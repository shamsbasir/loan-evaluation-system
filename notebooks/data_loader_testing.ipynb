{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9f978ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install torch transformers accelerate ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5ac0f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b18ec466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88257ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "\n",
    "def split_prompt_completion(messages):\n",
    "    \"\"\"\n",
    "    Given a list of messages with roles (system, user, assistant),\n",
    "    return a tuple: (messages_without_assistant, assistant_response)\n",
    "    \"\"\"\n",
    "    messages_without_assistant = []\n",
    "    assistant_response = None\n",
    "\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            assistant_response = message[\"content\"]\n",
    "        else:\n",
    "            messages_without_assistant.append(message)\n",
    "\n",
    "    return messages_without_assistant, assistant_response\n",
    "\n",
    "\n",
    "def generate_input_output_pair(prompt, target_responses, tokenizer):\n",
    "    # Apply the chat template to each prompt\n",
    "    chat_templates = tokenizer.apply_chat_template(prompt, continue_final_message=True,tokenize=False)\n",
    "\n",
    "    # Append assistant response + EOS token to each prompt\n",
    "    full_response_text = [\n",
    "        chat_template + \" \" + target_response + tokenizer.eos_token\n",
    "        for chat_template, target_response in zip(chat_templates, target_responses)\n",
    "    ]\n",
    "\n",
    "    # Tokenize the full input (prompt + response)\n",
    "    input_ids_tokenized = tokenizer(\n",
    "        full_response_text,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=False\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    # Tokenize only the responses (with EOS)\n",
    "    labels_tokenized = tokenizer(\n",
    "        [\" \" + response + tokenizer.eos_token for response in target_responses],\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        max_length=input_ids_tokenized.shape[1]\n",
    "    )[\"input_ids\"]\n",
    "    \n",
    "\n",
    "    # Replace padding tokens in labels with -100 so they are ignored in loss\n",
    "    labels_tokenized_fixed = torch.where(\n",
    "        labels_tokenized != tokenizer.pad_token_id,\n",
    "        labels_tokenized,\n",
    "        torch.tensor(-100)\n",
    "    )\n",
    "    labels_tokenized_fixed[:,-1] = tokenizer.pad_token_id  # Ensure last token is pad token\n",
    "\n",
    "    # Left shift input_ids (remove last token) & right shift labels (remove first token)\n",
    "    input_ids_tokenized_left_shifted = input_ids_tokenized[:, :-1]\n",
    "    labels_tokenized_right_shifted = labels_tokenized_fixed[:, 1:]\n",
    "\n",
    "    # Create attention mask for the shifted input\n",
    "    attention_mask = (input_ids_tokenized_left_shifted != tokenizer.pad_token_id)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids_tokenized_left_shifted,\n",
    "        \"labels\": labels_tokenized_right_shifted,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }\n",
    "\n",
    "\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, jsonl_path: str, tokenizer, max_length: int = 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.data = []\n",
    "        \n",
    "        with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                try:\n",
    "                    data_point = json.loads(line.strip())\n",
    "                    messages, response = split_prompt_completion(data_point[\"messages\"])\n",
    "                    if messages and response:\n",
    "                        self.data.append((messages, response))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Skipping line {line_num}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        print(f\"Loaded {len(self.data)} conversation pairs from {jsonl_path}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        messages, response = self.data[idx]\n",
    "        \n",
    "        # Use your existing function to generate the input-output pair\n",
    "        batch = generate_input_output_pair([messages], [response], self.tokenizer)\n",
    "        \n",
    "        # Extract the first (and only) item from the batch\n",
    "        input_ids = batch[\"input_ids\"][0]\n",
    "        labels = batch[\"labels\"][0]\n",
    "        attention_mask = batch[\"attention_mask\"][0]\n",
    "        \n",
    "        # Truncate to max_length if needed\n",
    "        if len(input_ids) > self.max_length:\n",
    "            input_ids = input_ids[:self.max_length]\n",
    "            labels = labels[:self.max_length]\n",
    "            attention_mask = attention_mask[:self.max_length]\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels, \n",
    "            \"attention_mask\": attention_mask\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch: List[Dict[str, torch.Tensor]], tokenizer):\n",
    "    \"\"\"Custom collate function to handle variable length sequences.\"\"\"\n",
    "    # Find max length in batch\n",
    "    max_len = max(len(item[\"input_ids\"]) for item in batch)\n",
    "    \n",
    "    input_ids = []\n",
    "    labels = []\n",
    "    attention_mask = []\n",
    "    \n",
    "    for item in batch:\n",
    "        # Current lengths\n",
    "        curr_len = len(item[\"input_ids\"])\n",
    "        pad_len = max_len - curr_len\n",
    "        \n",
    "        if pad_len > 0:\n",
    "            # Pad input_ids with pad_token_id\n",
    "            padded_input = torch.cat([\n",
    "                item[\"input_ids\"],\n",
    "                torch.full((pad_len,), tokenizer.pad_token_id, dtype=item[\"input_ids\"].dtype)\n",
    "            ])\n",
    "            \n",
    "            # Pad attention_mask with 0s\n",
    "            padded_attention = torch.cat([\n",
    "                item[\"attention_mask\"],\n",
    "                torch.zeros(pad_len, dtype=item[\"attention_mask\"].dtype)\n",
    "            ])\n",
    "            \n",
    "            # Pad labels with -100\n",
    "            padded_labels = torch.cat([\n",
    "                item[\"labels\"],\n",
    "                torch.full((pad_len,), -100, dtype=item[\"labels\"].dtype)\n",
    "            ])\n",
    "        else:\n",
    "            padded_input = item[\"input_ids\"]\n",
    "            padded_attention = item[\"attention_mask\"] \n",
    "            padded_labels = item[\"labels\"]\n",
    "        \n",
    "        input_ids.append(padded_input)\n",
    "        labels.append(padded_labels)\n",
    "        attention_mask.append(padded_attention)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": torch.stack(input_ids),\n",
    "        \"labels\": torch.stack(labels),\n",
    "        \"attention_mask\": torch.stack(attention_mask)\n",
    "    }\n",
    "\n",
    "\n",
    "class CollateFn:\n",
    "    \"\"\"Picklable collate function class.\"\"\"\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        return collate_fn(batch, self.tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b66243b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12000 conversation pairs from ../data/train.jsonl\n",
      "Loaded 1500 conversation pairs from ../data/val.jsonl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "model_id = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "local_dir = \"../models/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=local_dir)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ConversationDataset(\"../data/train.jsonl\", tokenizer, max_length=1024)\n",
    "val_dataset = ConversationDataset(\"../data/val.jsonl\", tokenizer, max_length=1024)\n",
    "\n",
    "# Create picklable collate function\n",
    "collate_func = CollateFn(tokenizer)\n",
    "\n",
    "\n",
    "# Number of workers for DataLoader\n",
    "num_workers = 0  # Adjust based on your system, 0 for no multiprocessing\n",
    "\n",
    "# Create dataloaders directly\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_func,\n",
    "    pin_memory=True if num_workers > 0 else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_func,\n",
    "    pin_memory=True if num_workers > 0 else False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e39af997",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14ec9420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 423]), torch.Size([4, 423]), torch.Size([4, 423]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_ids\"].shape, batch[\"attention_mask\"].shape, batch[\"labels\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd90013e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128006,   9125,  ...,   2759,   1210,     92],\n",
       "        [128000, 128006,   9125,  ..., 128004, 128004, 128004],\n",
       "        [128000, 128006,   9125,  ..., 128004, 128004, 128004],\n",
       "        [128000, 128006,   9125,  ..., 128004, 128004, 128004]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d3ce4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -100,   -100,   -100,  ...,   1210,     92, 128004],\n",
       "        [  -100,   -100,   -100,  ...,   -100,   -100,   -100],\n",
       "        [  -100,   -100,   -100,  ...,   -100,   -100,   -100],\n",
       "        [  -100,   -100,   -100,  ...,   -100,   -100,   -100]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be0671ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  ...,  True,  True,  True],\n",
       "        [ True,  True,  True,  ..., False, False, False],\n",
       "        [ True,  True,  True,  ..., False, False, False],\n",
       "        [ True,  True,  True,  ..., False, False, False]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d651575f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329ea5cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
